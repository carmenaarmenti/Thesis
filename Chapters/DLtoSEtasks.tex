\chapter{Deep Learning Applications to Software Engineering tasks}
The advent of deep learning (DL) has fundamentally changed the landscape of modern software and in order to cope with 
the increasing complexity of digital system programming, deep learning techniques have recently been proposed 
to enhance software deployment by analysing source code for different purposes, ranging from performance improvement 
to debugging and security assessment. Driven by the success of deep learning in data mining and pattern 
recognition, recent years witnessed an increasing trend for researchers to integrate deep learning with software engineering (SE) 
tasks. In the most typical SE tasks, deep learning helps to generate or summarize source code, predict defects in software, extract requirements from natural language text, 
and many more tasks.
Generally, a DL system is made of several interconnected computational atomic units that form \textit{layers}
which perform mathematical transformations, according to sets of learnable parameters, on input data. These architectures
can be trained for specific tasks updating the parameters according to model configuration on a specific set of training data. \\ Over the years, 
deep learning has developed advancements in many complex tasks often associated with artificial intelligence, within software engineering 
is now comprehended. DL is intertwined with SE, in fact, DL techniques allow to automate or improve existing software 
development tasks nowadays. 

\section{State of the art}
Given the effectiveness by which DL systems are able to learn representations from large data corpora, there is 
ample opportunity to leverage DL techniques to help automate or improve a wide range of code related tasks. 
Software engineering research investigates questions related to the design, development, maintenance, testing and evolution 
of software systems. Previously, the software engineering community has applied traditional machine learning 
techniques to identify interesting patterns and unique relationships within the data to automate or enhance many tasks typically 
performed by developers. Due to recent improvements in computational power and the amount of memory available 
in modern computer architectures, the rise of deep learning has led to a new class of learning algorithms suited for large datasets.
Deep learning represents a fundamental shift in the way by which machines learn patterns from data by automatically extracting salient features
for a given computational task, as opposed to relying upon human intuition. Given the immense amount of data in software repositories that 
can serve as training data, deep learning techniques have ushered in advancements across a range of tasks in software engineering research, including
automatic software fixing, code suggestion, defect prediction, feature location, among many others. This field of research shows clear potential for
transforming the manner by which a variety of specific software development task are performed.

\section{Software Engineering related works}
Taking as reference the piece of work published by \cite{barchi2022deep}, in this section some of the ML and
DL techniques to ssource code analysis are presented, briefly described in chronological order by 
year of publication.\\ 
Between 2013 and 2015, machine learning techniques were explored on high-level code, starting from a manual 
definition of features to the first deep learning applicactions capable of extracting features from code on their own.
The techniques used are strongly inspired by the background of natural language processing 
- being \textit{most software natural} \cite{hindle2016naturalness} since it is created by human beings - and include n-grams, decision threes, 
and recurrent neural networks (RNN). RNNs in particular have been extensively tested to assess their effectiveness;
Rayachev \textit{et al.} \cite{raychev2014code} proposed a code completion strategy that compares n-gram model and recurrent neural networks. 
They implement a technique that first extract sequences of API calls from the dataset, then apply n-gram to these sequences, and finally use the RNN 
to take the last word in the sequence as input and uses one-hot-encoding to predict probabilities for the most likely next word.
They argue that the n-gram technique can discover regularities between the last \textit{n} - 1 elements of fucntion-calls sequences, whereas RNN can discover
relations at longer distances.\\
Zaremba \textit{et al.} in \cite{zaremba2014learning} try instead to test the limits of Long Short Term Memory (LSTM) based networl with a task 
considered difficult. Specifically, given a code fragment, inferring the result as if one was running the code. They 
defined a simple class of programs and used a LSTM cell in a sequence-to-sequence model to obtain the result of the code execution. 
They reached 99\% of accuracy. The LSTM network receives direct input of the characters, thus it was not necessary for them to develop a complex 
embedding layer.\\
\newline
Shortly after, between 2016 and 2018, ML techniques for code started to be explored more in depth. Together with the RNNs and their variants,
such as LSTM and GRU - Gated Recurrent Units - cells, complex model based on a sequence-to-sequence structure emerged \cite{iyer2016summarizing},
\cite{bhatia2016automated},
alongside an adaptation of CNN on tree structures \cite{mou2016convolutional}, \cite{bui2018cross}
and  the introduction of a new neural network structure: the Graph Neural Network (GNN) \cite{allamanis2017learning}, \cite{scarselli2008graph}. The techniques were applied on 
Sequences of Tokens extracted from the source code, Abstract Syntax Tree \cite{mou2016convolutional} and API call sequences. 
The problems addressed span from code description and code comprehension, to syntax error recognition \cite{santos2018syntax}
and code fixing as well.\\
\newline
Recently, the topic has become more mature and more works have tried to exploit the 
Encoder-Decoder structure \cite{alon2018code2seq}, the GNN models \cite{allamanis2021self}, \cite{brauckmann2020compiler}, or other models 
based on hierarchical structures. The problems addressed are, for instance: function and 
type inferring \cite{alon2019code2vec}, \cite{pradel2020typewriter}, performance prediction \cite{mendis2019ithemal}, and loop-optimization \cite{haj2020neurovectorizer}, \cite{brauckmann2020compiler}.\\
\newline
The topic described approaches code analysis with machine learning and deep learning. However, 
the spectrum of problems in these areas is actually very broad and the works mentioned lead to the 
necessity to apply ML techniques to approach code related tasks. In the following sections, tree code related
tasks are described thoroughly.

\subsection{Bug fixing task}
Currently, there are millions of open source projects with numerous bug fixes available in code repositories
thanks to the proliferation of software development. This can be leveraged to learn how to fix common programming bugs. 
Localizing and fixing bugs is known to be an effort-prone and time-consuming task for software developers.
So, to support programmers in this common activity, researchers have proposed a number of approaches aimed to automatically repairing programs.
The work we took as reference is motivated by three main considerations. First of all, automated repair approaches are based on a limited and hand-crafted
set of transformations or fitting patterns. In the second place, the work done by \cite{} shows that the past history of 
existing projects can be successfully leveraged to understand what a \textit{meaningful} program repair patch is. Finally, 
several works have recently demonstrated the capability of advanced machine learning techniques, such as deep learning, to learn from relatively large software engineering
datasets. In the work, the authors expanded upon the original idea of learning bug-fixes and evaluate the suitability of a Neural Machine Translation to automatically
generate patches for buggy code.
%Each instance of the dataset is a pair $(m_b, m_f)$, where $m_b$ is a buggy code component and $m_f$ is the
%corresponding fixed code. These BFPs were used to train the NMT model, allowing it to learn the translation
%from the buggy to the fixed method, thus being able to generate fixing patches.


\subsection{Code summarization task}
Automatic source code summarization is the task of generating short natural language description 
for source code \cite{Leclair2020}. The idea is that a brief description allows programmers to understand
what a chunk of code does and what is the purpose of the program by and large, without necessarily read the code 
itself.\\

\subsection{Log generation task}