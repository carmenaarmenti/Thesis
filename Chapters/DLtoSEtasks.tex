\chapter{Deep Learning Applications to Software Engineering tasks}
The advent of deep learning (DL) has fundamentally changed the landscape of modern software and in order to cope with 
the increasing complexity of digital system programming, deep learning techniques have recently been proposed 
to enhance software deployment by analysing source code for different purposes, ranging from performance improvement 
to debugging and security assessment. Driven by the success of deep learning in data mining and pattern 
recognition, recent years witnessed an increasing trend for researchers to integrate deep learning with software engineering (SE) 
tasks. In the most typical SE tasks, deep learning helps to generate or summarize source code, predict defects in software, extract requirements from natural language text, 
and many more tasks.
Generally, a DL system is made of several interconnected computational atomic units that form \textit{layers}
which perform mathematical transformations, according to sets of learnable parameters, on input data. These architectures
can be trained for specific tasks updating the parameters according to model configuration on a specific set of training data. \\ Over the years, 
deep learning has developed advancements in many complex tasks often associated with artificial intelligence, within software engineering 
is now comprehended. DL is intertwined with SE, in fact, DL techniques allow to automate or improve existing software 
development tasks nowadays. 

\section{State of the art}
Given the effectiveness by which DL systems are able to learn representations from large data corpora, there is 
ample opportunity to leverage DL techniques to help automate or improve a wide range of code related tasks. 
Software engineering research investigates questions related to the design, development, maintenance, testing and evolution 
of software systems. Previously, the software engineering community has applied traditional machine learning 
techniques to identify interesting patterns and unique relationships within the data to automate or enhance many tasks typically 
performed by developers. Due to recent improvements in computational power and the amount of memory available 
in modern computer architectures, the rise of deep learning has led to a new class of learning algorithms suited for large datasets.
Deep learning represents a fundamental shift in the way by which machines learn patterns from data by automatically extracting salient features
for a given computational task, as opposed to relying upon human intuition. Given the immense amount of data in software repositories that 
can serve as training data, deep learning techniques have ushered in advancements across a range of tasks in software engineering research, including
automatic software fixing, code suggestion, defect prediction, feature location, among many others. This field of research shows clear potential for
transforming the manner by which a variety of specific software development task are performed.

\section{Software Engineering related works}
Taking as reference the piece of work published by \cite{barchi2022deep}, in this section some of the ML and
DL techniques to ssource code analysis are presented, briefly described in chronological order by 
year of publication.\\ 
Between 2013 and 2015, machine learning techniques were explored on high-level code, starting from a manual 
definition of features to the first deep learning applicactions capable of extracting features from code on their own.
The techniques used are strongly inspired by the background of natural language processing 
- being \textit{most software natural} \cite{hindle2016naturalness} since it is created by human beings - and include n-grams, decision threes, 
and recurrent neural networks (RNN). RNNs in particular have been extensively tested to assess their effectiveness;
Rayachev \textit{et al.} \cite{raychev2014code} proposed a code completion strategy that compares n-gram model and recurrent neural networks. 
They implement a technique that first extract sequences of API calls from the dataset, then apply n-gram to these sequences, and finally use the RNN 
to take the last word in the sequence as input and uses one-hot-encoding to predict probabilities for the most likely next word.
They argue that the n-gram technique can discover regularities between the last \textit{n} - 1 elements of fucntion-calls sequences, whereas RNN can discover
relations at longer distances.\\
Zaremba \textit{et al.} in \cite{zaremba2014learning} try instead to test the limits of Long Short Term Memory (LSTM) based networl with a task 
considered difficult. Specifically, given a code fragment, inferring the result as if one was running the code. They 
defined a simple class of programs and used a LSTM cell in a sequence-to-sequence model to obtain the result of the code execution. 
They reached 99\% of accuracy. The LSTM network receives direct input of the characters, thus it was not necessary for them to develop a complex 
embedding layer.\\
\newline
Shortly after, between 2016 and 2018, ML techniques for code started to be explored more in depth. Together with the RNNs and their variants,
such as LSTM and GRU - Gated Recurrent Units - cells, complex model based on a sequence-to-sequence structure emerged \cite{iyer2016summarizing},
\cite{bhatia2016automated},
alongside an adaptation of CNN on tree structures \cite{mou2016convolutional}, \cite{bui2018cross}
and  the introduction of a new neural network structure: the Graph Neural Network (GNN) \cite{allamanis2017learning}, \cite{scarselli2008graph}. The techniques were applied on 
Sequences of Tokens extracted from the source code, Abstract Syntax Tree \cite{mou2016convolutional} and API call sequences. 
The problems addressed span from code description and code comprehension, to syntax error recognition \cite{santos2018syntax}
and code fixing as well.\\
\newline
Recently, the topic has become more mature and more works have tried to exploit the 
Encoder-Decoder structure \cite{alon2018code2seq}, the GNN models \cite{allamanis2021self}, \cite{brauckmann2020compiler}, or other models 
based on hierarchical structures. The problems addressed are, for instance: function and 
type inferring \cite{alon2019code2vec}, \cite{pradel2020typewriter}, performance prediction \cite{mendis2019ithemal}, and loop-optimization \cite{haj2020neurovectorizer}, \cite{brauckmann2020compiler}.\\
\newline
The topic described approaches code analysis with machine learning and deep learning. However, 
the spectrum of problems in these areas is actually very broad and the works mentioned lead to the 
necessity to apply ML techniques to approach code related tasks. In the following sections, tree code related
tasks are described thoroughly.

\subsection{Bug fixing task}
Currently, there are millions of open source projects with numerous bug fixes available in code repositories
thanks to the proliferation of software development. This can be leveraged to learn how to fix common programming bugs. 
Localizing and fixing bugs is an effort-prone and time-consuming task for software developers;
thus, to support programmers in this common activity, researchers have proposed a number of approaches aimed to automatically repaire programs.
The work we took as reference is motivated by three main considerations. 
First of all, automated repair approaches are based on a limited and hand-crafted
set of transformations or fitting patterns. 
In the second place, the work done by \cite{le2016history} shows that the past history of 
existing projects can be successfully leveraged to understand what a \textit{meaningful} program repair patch is. 
Finally, several works have recently demonstrated the capability of advanced machine learning techniques, such as deep learning, to learn from relatively large software engineering
datasets. 
In the work, the authors expanded upon the original idea of learning bug-fixes and evaluate the suitability of a NMT network to automatically
generate patches for buggy code.
Actually, software developers can access to plenty of change history and 
bug-fixing commits from a large number if software projects, from GitHub for example, and
a machine-learning based approach can use this data to learn about 
bug-fixing activities. In fact, automatically learning from bug-fixes provides the 
chance to emulate real patches written by developers. 
Moreover, the scholars harness the power of NMT - originally meant for translation purposes -
to attempt indeed the \textit{translation} of buggy code into fixed code emulating 
the combination of Abstract Syntax Tree (AST) operations performed in developers written patches.\\
\newline
\textbf{The Approach.} They mined from GitHub Archive \cite{grigorik2012github} bug-fixing commits 
from thousands of repositories, considering every public GitHub event between March 2011 and 
October 2017. For each bug-fixing commit, the authors extracted the source code before and 
after the bug-fix, discarding commits related to non-Java files.
In this way, they 
collected method-level bug-fixing pairs (BFPs), that is the pre and post commit code, 
ending having ~ 2.3\(M\) bug-fixing commits.
Each instance of the dataset is a pair $(m_b, m_f)$, where $m_b$ is a buggy code component and $m_f$ is the
corresponding fixed code. These BFPs were used to train the NMT model, allowing it to learn the translation
from the buggy to the fixed method, thus being able to generate fixing patches. Since learning bug-fixing
patterns is highly challengin by working at the level of raw source code, they abstracted 
each BFP in isolation using a Java lexer, a parser and a list of idioms to represent each buggy and fixed method 
within a BFP as a stream of tokens. After that, they filtered out: (i) BFPs that contained lexical or syntactic 
errors - either because of lexer or parser failure in processing them; (ii) the pairs whose buggy 
and fixed abstracted code resulted in equal strings; (iii) BFPs that performed more than 100 AST actions between the buggy 
and fixed version - so to eliminate outliers of the distribution which could have hindered
the learning process. Subsequently, they considered BFPs based on their size measured
in the number of tokens and disregarded methods longer than 100 tokens and focused on two sizes thus creating 
two datasets: \(BFP_small\) composed by BFPs with maximum 50 tokens and \(BFP_medium\)
composed by BFPs made up of a token number between 50 and 100.
Then, given those datasets they used their instances to train an Encoder-Decoder
model, where the instances were randomly partitioned into: training (80\%), validation (10\%) and 
test (10\%) sets. They also made sure to discard any possible duplicate.\\
The experimented models are based on an Recurrent Neural Network (RNN) Encoder-Decoder
architecture with attention mechanism, frequently used in Neural Machine Translation. 
This kind of model is composed by two
dominant components:

\begin{itemize}
    \item a RNN Encoder, which encodes a sequence of terms \textbf{x} into a vector representation;
    \item a RNN Decoder, which decodes the vector representation into another sequence of terms \textbf{y}.
\end{itemize}
The model learning is based on a conditional distribution, where the output sequence of terms is conditioned
by the input sequence: \(P(y_1,...,y_m|x_1,...,x_n)\), and where \(m\) and \(n\) not necessarly have to have the same length.
The Encoder takes as input a sequence \textbf{x}\(= (x_1,...,x_n)\) and produces
a sequence of states \textbf{h}\(= (h_1,...,h_n)\). The framework relies on a bi-directional
RNN Encoder, which is composed by a backward and a forward RNN, where both are able to create representations taking into account
past and future inputs. Specifically, each state \(h_i\) is the concatenation of the states produced by 
the two RNNs when reading the sequence not only in a forward but also in a backward manner.\\
The RNN Decoder computes the probability of a target sequence \textbf{y}\(= (y_1,...,y_n)\) given \textbf{h}. 
The probability
of each output term \(h_i\) is computed based on:
\begin{itemize}
    \item the recurrent state \(s_i\) in the Decoder;
    \item the previous \(i - 1\) terms \((y_1,...,y_i-1)\);
    \item a context vector \(c_i\), which constitutes the attention mechanism.
\end{itemize}
The vector \(c_i\) is a weighted average of the states in \textbf{h}, where the weights associated 
to each state allow the model to pay more attention to some parts of the input sequence than to others:
\[c_i = \sum_{t=1}^n a_{it} h_t\]
Precisely, the weigth \(a_{it}\) defines how much the model should take into consideration the term of the sequence in input \(x_i\)
when predicting the target term \(y_t\). Encoder and Decoder are simultaneously trained - instead of sequentially - by minimizing
the negative log likelihood of the target terms, using stochastic gradient descent.\\
In terms of hyperparameter search, for both models build on the \(BFP_small\) and \(BFP_medium\) datasets,
the scholars tested 10 encoder-decoder architecture configurations. The configurations 
tested different combinations of RNN cells - LSTM and GRU; number of layers - 1, 2,3; units for both the encoder
and the decoder - 256, 512; embedding size - 256, 512.
Bucketing and padding was used to deal with the variable length of the sequences.
They trained their models for a maximum of 60k epochs and selected the model checkpoint before 
overfitting the training data. Moreover, to guide the selection of the best configuration, they 
used the loss function computed on the validation set; the results were computed on the test set.
Once the model was trained, it was evaluated against the test set of unseen buggy code and apart from using the classic
greedy decoding approach that selects at each timestep the output with highest probability, they also used Beam Search decoding
in order to generate up to 50 potential patches for a given buggy code.\\
\newline
The scope of their work was to answer to 3 research questions: (i) if the NMT was a viable approach 
to learn how to fix code; (ii) what kind of operations were perfomed by the models; (iii) what was the training 
and inference time of the models. The results obtained showed that: (i) training a model both on small BFPs and medium
BFPs, NMT showed to be a viable approach to learn how to fix code; (ii) the models exhibited a very high syntactic 
correctness of the generate patches ranging between 99\% and 82\% and the learned operation are the most representative ones 
giving the chance to theoretically fix a large percentage of bugs; (iii) after training for less than 15 hours,
the models were able to generate 50 candidate patches for a single bug in less than a second.\\
\newline
In the end, they empirically investigated the potential of NMT to generate
candidate patches that are identical to those implemented by developers. Their results indicate 
that the trained NMT model can successfully predict the fixed code - given the buggy code - in 9\% of the cases (as for the small dataset) and in 
3\% of the cases (as for the medium dataset).

\subsection{Code summarization task}
Automatic source code summarization is the task of generating short natural language description 
for source code \cite{Leclair2020}. The idea is that a brief description allows programmers to understand
what a chunk of code does and what is the purpose of the program by and large, without necessarily read the code 
itself.\\

\subsection{Log generation task}