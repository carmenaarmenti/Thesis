\chapter{Analysis of results}
In the following section the results 
achieved by NMT Encoder-Decoder with curriculum learning and
by the baselines for the three tasks we considered are reported.
We used different metrics for each of the tasks, depending on the metric used in the works
that introduced the baselines.

\section{Bug fixing task}
To begin with, as assessed by Tufano \textit{et al.} \cite{Tufano2019} we used 
perfect predictions as a metric to evaluate the model performances. In the paper taken as reference
to test curriculum learning on, the authors differentiated 2 types of datasets; (i) the first one constituted by methods
whose length was up to 50 tokens and (ii) methods with length between 50 and 100 tokens. 
However, as stated in the previous sections
we considered the merging of the two datasets, 
therefore firstly we reproduced the canonical learning with the merged dataset and then we tested curriculum learning
approach.
Table~\ref{table:pp_bugfixing} reports the percentage %and numbers (?)
of bug fixing pairs correctly predicted
by the models for different beam sizes.
\begin{table}[h!]
    \centering
    \begin{tabular}{l|r|r}
    BEAM & Baseline & Baby-step\\ [0.5ex]
    \hline 
    1 & 5.22 \% & 5.34 \%\\ 
    5 & 13.00 \% & 19.41 \%\\
    10 & 16.55 \% & 25.16 \%\\
    15 & 18.38 \% & 28.64 \%\\
    20 & 19.60 \% & 30.84 \%\\
    25 & 20.51 \% & 32.39 \%\\
    30 & 21.29 \% & 33.69 \%\\
    35 & 21.93 \% & 34.93 \%\\
    40 & 22.26 \% & 35.72 \%\\
    45 & 22.73 \% & 36.47 \%\\
    50 & 23.02 \% & 37.39 \%\\ [1ex]
    \end{tabular}
    \caption{Models' performances}
    \label{table:pp_bugfixing}
\end{table}
Increasing the beam size, and generating more candidate patches accordingly,
the percentages of BFPs for which the models can perfectly generate the corresponding
fixed code - starting from the input buggy code - increases. If the baseline model can predict
the fixed code of 5.22\% of the BFPs with only one attempt, the same model together with 
curriculum learning approach performs predicting 5.34\% of the same BFPs. It is a decent 
better result,
but looking at bigger beam sizes, the model with curriculum learning performs 
definetly better, almost triplicating the percentage of perfect predictions when 5 patches are generated,
reaching 37.39\% when 50 candidate patches are considered. Overall, it can be seen that
the improvement margin is constant.\\
On a second evaluation, we carried on a complementary analysis based on perfect predictions
obtained when the model generated 10 candidate patches for each prediction. 
As can be seen 
in Table~\ref{table:pp_bugfixing_overlap}, the combination between the two models 
leads to a reasonable percentage of perfect predictions, i.e. 42.38\%. However, the model 
with curriculum learning approach performs even better and on its own. Indeed, 44.63\% of BFPs are correctly predicted
only by the model with baby-step. On the other hand, only 12.97\% of perfect predictions are from the baseline.
\begin{table}[h!]
    \centering
    \begin{tabular}{l|r|r|r}
    \(Dataset (d)\) & \(Shared_d\) & \(OnlyBL_d\) & \(OnlyCL_d\)\\ [0.5ex]
    \hline 
    \(BF_{all}\) & 42.38 \% & 12.97 \% & 44.63 \%\\  [1ex]
    \end{tabular}
    \caption{Overlapping metrics: baseline and curriculum learning models.}
    \label{table:pp_bugfixing_overlap}
\end{table}
This result not only indicates that the two approaches are complementary for
the bug-fixing task, but also that the model with curriculum learning reports even 
better outcomes. Considering that the approach implemented is one of the easiest one,
those results recall the need to better tuning the approach with the aim of 
other improvements in the ability of such a model to exploit the knowledge acquired on 
this specific task. 

\section{Code summarization task}
In the study taken as reference the authors used different neural networks to 
assess the quality of their work. So, the comparison between our results and theirs
is not really straigh - forward. However, as said before, we reproduced the experiment on a baseline model,
where baseline means the model without any curriculum learning approach applied.
The experiment on the baseline model totalized a bleu score of 13.70 that is ...
\begin{table}[h!]
    \centering
    \begin{tabular}{l|r|r|r|r|r}
     & BLEU-A & BLEU-1 & BLEU-2 & BLEU-3 & BLEU-4\\ [0.5ex] 
     \hline
     Baseline & 13.70 & 38.8 & 18.4 & 10.6 & 7.1\\  
     Baby-step & 13.29 & 38.0 & 17.8 & 10.2 & 7.0\\ [1ex]
     \end{tabular}
    \caption{BLEU values summary .}
    \label{table:1}
    \end{table}
On the other hand, the bleu value after the experiment on the model together with the curriculum approach
scored a value of 13.29, which is lower than the results achieved by the baseline.
This means that in this case the curriculum approach does not work in the view of collect better results.
However, it must be said that the models used are different; this might mean that the differences observed
may depend on the models used and not on the technique applied.\\
Also on perfect predictions we do not have better results, but we can observe - as showed in Table~\ref{table:2} - that the values are not 
that much distant one from another.

\begin{table}[h!]
    \centering
    \begin{tabular}{l|r|r} 
    %\multicolumn{3}{c}{Perfect predictions} \\
    & Baseline & Baby-step\\ [0.5ex] 
    \hline
    BEAM 1 & 4.66 \% & 4.64 \%\\  
    BEAM 5 & 6.68 \% & 6.53 \% \\ 
    BEAM 10 & 7.75 \% & 7.52 \%\\
    BEAM 15 & 8.36 \% & 8.07 \%\\
    BEAM 20 & 8.84 \% & 8.51 \%\\
    BEAM 25 & 9.17 \% & 8.83 \%\\
    BEAM 30 & 9.47 \% & 9.06 \%\\
    BEAM 35 & 9.74 \% & 9.29 \%\\
    BEAM 40 & 9.90 \% & 9.49 \%\\
    BEAM 45 & 10.06 \%& 9.67\%\\
    BEAM 50 & \%& 9.83\%\\ [1ex]
    \end{tabular}
    \caption{Perfect predictions summary.}
    \label{table:2}
\end{table}
%\section{Log generation task}